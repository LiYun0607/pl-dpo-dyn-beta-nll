<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PL-DPO-NLL: Safety-Aligned Autonomous Driving</title>
    <style>
        :root {
            --primary: #2563eb;
            --secondary: #1e40af;
            --bg: #f8fafc;
            --text: #1e293b;
            --muted: #64748b;
            --border: #e2e8f0;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: var(--text);
            background: var(--bg);
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 2rem;
        }

        header {
            text-align: center;
            padding: 3rem 0;
            background: linear-gradient(135deg, var(--primary) 0%, var(--secondary) 100%);
            color: white;
            margin-bottom: 2rem;
        }

        h1 {
            font-size: 1.8rem;
            font-weight: 700;
            margin-bottom: 1rem;
            line-height: 1.3;
        }

        .venue {
            font-size: 1.1rem;
            opacity: 0.9;
            margin-bottom: 1.5rem;
        }

        .authors {
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }

        .affiliations {
            font-size: 0.9rem;
            opacity: 0.8;
        }

        .buttons {
            display: flex;
            justify-content: center;
            gap: 1rem;
            margin-top: 1.5rem;
            flex-wrap: wrap;
        }

        .btn {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.6rem 1.2rem;
            border-radius: 8px;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s;
        }

        .btn-primary {
            background: white;
            color: var(--primary);
        }

        .btn-primary:hover {
            background: #f1f5f9;
            transform: translateY(-2px);
        }

        .btn-disabled {
            background: rgba(255,255,255,0.3);
            color: white;
            cursor: not-allowed;
        }

        section {
            background: white;
            border-radius: 12px;
            padding: 2rem;
            margin-bottom: 1.5rem;
            box-shadow: 0 1px 3px rgba(0,0,0,0.1);
        }

        h2 {
            font-size: 1.3rem;
            color: var(--primary);
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 2px solid var(--border);
        }

        .abstract {
            text-align: justify;
            color: var(--text);
        }

        .teaser {
            text-align: center;
            margin: 1rem 0;
        }

        .teaser img {
            max-width: 100%;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }

        .teaser-caption {
            font-size: 0.9rem;
            color: var(--muted);
            margin-top: 0.5rem;
        }

        .coming-soon {
            display: flex;
            align-items: center;
            justify-content: center;
            padding: 3rem;
            background: #f1f5f9;
            border-radius: 8px;
            color: var(--muted);
            font-size: 1.1rem;
        }

        .coming-soon-badge {
            background: var(--primary);
            color: white;
            padding: 0.3rem 0.8rem;
            border-radius: 20px;
            font-size: 0.8rem;
            margin-left: 0.5rem;
        }

        .highlight {
            background: #dbeafe;
            padding: 1rem;
            border-radius: 8px;
            border-left: 4px solid var(--primary);
            margin: 1rem 0;
        }

        .highlight strong {
            color: var(--primary);
        }

        .contributions {
            list-style: none;
        }

        .contributions li {
            padding: 0.8rem 0;
            border-bottom: 1px solid var(--border);
            display: flex;
            align-items: flex-start;
            gap: 0.8rem;
        }

        .contributions li:last-child {
            border-bottom: none;
        }

        .contributions li::before {
            content: "✓";
            color: var(--primary);
            font-weight: bold;
            flex-shrink: 0;
        }

        footer {
            text-align: center;
            padding: 2rem;
            color: var(--muted);
            font-size: 0.9rem;
        }

        @media (max-width: 600px) {
            h1 { font-size: 1.4rem; }
            .container { padding: 1rem; }
            section { padding: 1.5rem; }
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1>Safety-Aligned Autonomous Driving via Plackett-Luce Preference Learning with NLL Regularization</h1>
            <div class="venue">RA-L 2025 (Under Review)</div>
            <div class="authors">
                Yun Li<sup>1</sup>, Ehsan Javanmardi<sup>1</sup>, Simon Thompson<sup>2</sup>, Manabu Tsukada<sup>1</sup>
            </div>
            <div class="affiliations">
                <sup>1</sup>The University of Tokyo &nbsp;&nbsp; <sup>2</sup>TIER IV, Inc.
            </div>
            <div class="buttons">
                <a href="#" class="btn btn-disabled">
                    <svg width="16" height="16" viewBox="0 0 24 24" fill="currentColor"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8l-6-6zm4 18H6V4h7v5h5v11z"/></svg>
                    Paper (Coming Soon)
                </a>
                <a href="#" class="btn btn-disabled">
                    <svg width="16" height="16" viewBox="0 0 24 24" fill="currentColor"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg>
                    Code (Coming Soon)
                </a>
                <a href="#" class="btn btn-disabled">
                    <svg width="16" height="16" viewBox="0 0 24 24" fill="currentColor"><path d="M20 6h-8l-2-2H4c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h16c1.1 0 2-.9 2-2V8c0-1.1-.9-2-2-2zm0 12H4V8h16v10z"/></svg>
                    Dataset (Coming Soon)
                </a>
            </div>
        </div>
    </header>

    <main class="container">
        <section>
            <h2>Abstract</h2>
            <p class="abstract">
                Safe autonomous driving demands that learned policies maintain high confidence in safe actions, yet this critical property is often violated during preference-based training. Vision-Language-Action (VLA) models have emerged as promising end-to-end driving agents, but they struggle with safety alignment due to imbalanced training data and <em>probability collapse</em>, where the likelihood of safe actions decreases during optimization. We propose <strong>Plackett-Luce DPO with NLL Regularization (PL-DPO-NLL)</strong>, a preference learning framework for safety-aligned driving. Our method introduces three synergistic innovations: (1) Plackett-Luce multi-preference ranking that learns from multiple rejected actions ranked by risk severity, (2) scene-adaptive β weighting that amplifies gradients for critical scenarios, and (3) NLL regularization that explicitly maintains chosen action probability. On CARLA closed-loop benchmark, PL-DPO-NLL achieves <strong>8.4% improvement</strong> in driving score with the highest route completion (65.9%) and infraction penalty (89.1%), outperforming DPO variants including IPO, EXO, BCO, and SPPO.
            </p>
        </section>

        <section>
            <h2>Key Results</h2>
            <div class="highlight">
                <strong>8.4% improvement</strong> in driving score over SFT baseline on CARLA closed-loop benchmark
            </div>
            <ul class="contributions">
                <li><strong>Highest Route Completion (65.9%)</strong> — NLL regularization prevents the model from "forgetting" successful driving patterns</li>
                <li><strong>Best Infraction Penalty (89.1%)</strong> — Plackett-Luce ranking enables fine-grained safety distinctions</li>
                <li><strong>Outperforms 6 DPO variants</strong> — Including IPO, EXO, BCO, SPPO, and standard DPO</li>
            </ul>
        </section>

        <section>
            <h2>Framework</h2>
            <div class="teaser">
                <!-- Replace with your actual framework figure -->
                <div class="coming-soon">
                    Framework figure coming soon
                    <span class="coming-soon-badge">After Acceptance</span>
                </div>
            </div>
            <p class="teaser-caption">
                Overview of our PL-DPO-NLL framework: frozen visual encoders, trainable LoRA adapter, and the training pipeline with Plackett-Luce ranking, scene-adaptive β, and NLL regularization.
            </p>
        </section>

        <section>
            <h2>Resources</h2>
            <div class="coming-soon">
                Code, checkpoints, and dataset will be released upon paper acceptance.
                <span class="coming-soon-badge">Coming Soon</span>
            </div>
        </section>

        <section>
            <h2>Citation</h2>
            <pre style="background: #f1f5f9; padding: 1rem; border-radius: 8px; overflow-x: auto; font-size: 0.85rem;">
@article{li2025pldponll,
  title={Safety-Aligned Autonomous Driving via Plackett-Luce
         Preference Learning with NLL Regularization},
  author={Li, Yun and Javanmardi, Ehsan and Thompson, Simon
          and Tsukada, Manabu},
  journal={IEEE Robotics and Automation Letters},
  year={2025},
  note={Under Review}
}</pre>
        </section>
    </main>

    <footer>
        <p>© 2025 PL-DPO-NLL Project. This page will be updated upon paper acceptance.</p>
    </footer>
</body>
</html>
